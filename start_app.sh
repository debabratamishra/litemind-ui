#!/bin/bash

#!/bin/bash

# LLMWebUI Application Startup Script
# This script ensures proper application startup and provides helpful information

echo "üöÄ Starting LLMWebUI Application..."
echo "=================================="

# Stop any existing containers
echo "üõë Stopping existing containers..."
docker-compose down 2>/dev/null || true

# Build the application
echo "üèóÔ∏è  Building application..."
docker-compose build --no-cache

# Start the application
echo "üöÄ Starting services..."
docker-compose up -d

# Wait a moment for containers to start
echo "‚è≥ Waiting for services to initialize..."
sleep 15

# Check container status
echo "üìä Container Status:"
docker-compose ps

# Test backend health
echo ""
echo "ü©∫ Testing Backend Health..."
BACKEND_HEALTH=$(docker exec llmwebui-backend curl -s http://localhost:8000/health 2>/dev/null || echo "Backend not ready")
if [[ "$BACKEND_HEALTH" == *"healthy"* ]]; then
    echo "‚úÖ Backend: HEALTHY"
else
    echo "‚ùå Backend: NOT READY - Check logs with: docker-compose logs backend"
fi

# Test frontend
echo "üñ•Ô∏è  Testing Frontend..."
if curl -s http://localhost:8501 >/dev/null 2>&1; then
    echo "‚úÖ Frontend: ACCESSIBLE"
else
    echo "‚ùå Frontend: NOT ACCESSIBLE - Check logs with: docker-compose logs frontend"
fi

echo ""
echo "üéâ Application startup complete!"
echo ""
echo "üåê Access your application at:"
echo "   üì± Frontend (Streamlit UI): http://localhost:8501"
echo "   üîß Backend API Documentation: Available via container"
echo ""
echo "üìã Management Commands:"
echo "   View all logs:      docker-compose logs -f"
echo "   View backend logs:  docker-compose logs backend"
echo "   View frontend logs: docker-compose logs frontend"
echo "   Stop application:   docker-compose down"
echo "   Restart:            docker-compose restart"
echo "   Container status:   docker-compose ps"
echo ""
echo "üîç Optional Services (for enhanced functionality):"
echo "   ‚Ä¢ Ollama (for local LLM models): https://ollama.ai/"
echo "     - Install: brew install ollama (macOS) or visit ollama.ai"
echo "     - Run: ollama serve"
echo "     - Pull models: ollama pull llama2"
echo ""
echo "   ‚Ä¢ vLLM (for high-performance inference): pip install vllm"
echo ""
echo "‚ú® Your LLM WebUI is ready to use!"
echo "   Start by visiting: http://localhost:8501"
